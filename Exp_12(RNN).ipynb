{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb8fec7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense\n",
    "\n",
    "# --- 1. Define the Training Data (Corpus) ---\n",
    "# A small, simple corpus for quick demonstration.\n",
    "text_corpus = \"\"\"\n",
    "Hello world\n",
    "This is a simple recurrent neural network example\n",
    "It can predict the next word\n",
    "\"\"\"\n",
    "\n",
    "print(\"--- Text Corpus Loaded ---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4cfab4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# --- 2. Data Preprocessing (Tokenization) ---\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([text_corpus])\n",
    "word_index = tokenizer.word_index\n",
    "total_words = len(word_index) + 1  # +1 for the padding token (0)\n",
    "\n",
    "# Convert text into a sequence of integer tokens\n",
    "token_list = tokenizer.texts_to_sequences([text_corpus])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77939f81",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# --- 3. Create Input Sequences (X) and Target Word (Y) ---\n",
    "# Create n-gram sequences: (word1, word2, word3) -> target (word4)\n",
    "input_sequences = []\n",
    "for i in range(1, len(token_list)):\n",
    "    n_gram_sequence = token_list[:i+1]\n",
    "    input_sequences.append(n_gram_sequence)\n",
    "\n",
    "# Pad sequences to ensure all inputs have the same length\n",
    "max_sequence_len = max([len(x) for x in input_sequences])\n",
    "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
    "\n",
    "# Separate Input (X) and Label (Y)\n",
    "X = input_sequences[:, :-1]\n",
    "y_int = input_sequences[:, -1]  # The last word is the target\n",
    "y = to_categorical(y_int, num_classes=total_words)\n",
    "\n",
    "print(f\"total_words: {total_words}, max_sequence_len: {max_sequence_len}\")\n",
    "print(f\"input: {X.shape}, target: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844b0201",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# --- 4. Build the SimpleRNN Model ---\n",
    "embedding_dim = 100  # Size of the word vector\n",
    "\n",
    "model = Sequential([\n",
    "    # Embedding layer: maps words to a dense vector space\n",
    "    Embedding(total_words, embedding_dim, input_length=max_sequence_len - 1),\n",
    "\n",
    "    # SimpleRNN layer: the recurrent core of the model\n",
    "    SimpleRNN(100),\n",
    "\n",
    "    # Dense output layer: predicts probability for each word in the vocabulary\n",
    "    Dense(total_words, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Display model structure\n",
    "print(\"\\n--- Model Summary ---\")\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79f0743",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "model.fit(X, y, epochs=5, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83a27b8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# --- 6. Prediction Function and Example ---\n",
    "def predict_next_word(seed_text, n_words):\n",
    "    \"\"\"Generates the next word(s) based on a starting text.\"\"\"\n",
    "    for _ in range(n_words):\n",
    "        # Convert seed text to sequence\n",
    "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "\n",
    "        # Pad the sequence to the required input length\n",
    "        token_list = pad_sequences([token_list], maxlen=max_sequence_len - 1, padding='pre')\n",
    "        # Predict the next word\n",
    "        # Predict the word index\n",
    "    predicted_probs = model.predict(token_list, verbose=0)\n",
    "    predicted_word_index = np.argmax(predicted_probs)\n",
    "\n",
    "    # Find the word corresponding to the index\n",
    "    output_word = \"\"\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == predicted_word_index:\n",
    "            output_word = word\n",
    "            break\n",
    "\n",
    "    # Append the predicted word and repeat\n",
    "    seed_text += \" \" + output_word\n",
    "\n",
    "    return seed_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d531adc8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Example Prediction\n",
    "print(\"\\n--- Prediction Example ---\")\n",
    "start_text = \"this is a simple\"\n",
    "predicted_sentence = predict_next_word(start_text, 2)\n",
    "\n",
    "print(f\"Start Text: '{start_text}'\")\n",
    "print(f\"Predicted Sentence: '{predicted_sentence}'\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
